一 15分
1a 极大后验MAP什么时候等于极大似然ML
	- 当假设空间H中的每个假设h为均匀分布，即P（hi）相等时
	
1b 和朴素贝叶斯有什么关系？细节忘了，求补充
	- 什么和朴素贝叶斯？朴素贝叶斯中，如果满足给定目标值后，各属性独立，则NB=MAP
	- 极大似然假设ML即最小误差平方假设
	- 一致学习都是极大后验假设MAP
	
2 过拟合，A1,A2，为什么的问题，见往年卷
	- 过拟合：对于假设h∈H，存在另外一个假设h’,使得在训练集上，h的错误率小于h’，但是在测试集上h的错误率大于h’，则说这个假设h过度拟合
	
3 集成学习对决策树和贝叶斯哪个效果好？为什么？往年题的KNN改成了贝叶斯
	- 对决策树更好，贝叶斯几乎不变，因为决策树是unnstable的

二 25分
1 ID3 K-MEANS KNN对噪声数据怎么处理的（或者不能处理的话为什么）
	- ID3和KNN，都当做正常数据处理
	- K-MEANS 没有lable，无监督，无法处理
	
2 上三者对训练样例全是正例的话会怎么样
	- ID3：直接得到只有一个叶子的决策树，叶子label为正
	- K-MEANS：无监督学习，没有影响
	- KNN：对于任意一个新的样题，投票获得的标签都会是正的
	
3 bagging和boosting的2点相同和2点不同，往年题。
	- 相同点：都能提高unstable的学习器的准确率
			  都是单种学习器
	- 不同点：bagging无权值，boosting有权值
		      bagging训练集使用boostrap采样，boosting每次训练集都相同

三 10分
0.4 0.47，h2有多大概率比h1好的往年题。给公式和正态分布表
	- 取d'=0.07.需求d>0的概率，即d>d'-0.07，即d'<d+0.07。d恰好为d’的均值，然后求d的标准差，d的方差为两个0.4、0.47对应的errorS方差的和。算出
	  标准差后，用0.07/标准差，得到的结果查表，得到对应的置信度。
	  方法在学习理论那节的PPT里有

四 15分
1 推mistake bound，得出结论是M≤（log(1/b)+klog(n)）/(log(2/1+b))这样的。。
    - 集成学习的PPT

2 求二维上圆（3）和三角形（7）的VC维，需要给出说明。
---------以下来自mythly（主要）和ejade（次要）的讨论结论-------
一维，实数轴上的点，用区间分，VC=2

二维，平面上的点
用直线/圆（强于直线，直径无穷大时可看成直线）VC=3（维数+1） 

直线易证，圆3时易证 4时对任意四个点找最小的外接圆，然后要圆上的至多三个点在圆内，其余点在圆外，矛盾。

用矩形/正方形 VC=4（维数*2）

4时易证，5时取最上最左最右最下的点在里面，剩下一个点在外面。

凸多边形 VC=维数*边数+1

对三角形简要证明思路。
证存在7可以时，举个正七边形，0个在里面1个在里面2个在里面3个在里面（以那三个为顶点画）都显然。剩下4567在里面，相当于任意0123在外面，比如3个在外面，三角形一条边割一个出去即可。
证任何8不可以时，首先考察任意8个点的凸包（包含点集中所有点的最小面积的凸多边形），如果有点在凸包内，那么要凸包上的点在里面，凸包里的点在外面，这显然是不可能的。
否则就是8个点都在凸包上。取不相邻的4个在里面。另外不相邻的4个就要在外面，由于在外面至少要在三角形一条边的外面，根据鸽笼原理，至少有两个点在同一边的外面。这样势必那两点间的应该在里面的点也会被切出去，矛盾了。。。

意思大致看看就行了嘛~格式很渣的>.<
所以凸45678变形都可以用上述证法以此类推。。明年考个凸四边形吧~

三维，根据推论
平面/球 4（维数+1）
超立方体（正方体，长方体）VC=6（维数*2）
证略
 
涉及到机器学习算法的比如用bool来表达决策树之类的往年题？一般都是∞
--------------------------------------------------------------

五 15分
n       F1(abc) F2      F3[0,1] result
1       a       T       0.2     yes
2       b       F       0.5     yes
3       a       F       0.7     yes
4       b       F       0.1     no
5       b       T       0.6     no
6       a       T       0.9     no
大致是这样，可能ab或者TF有错，反正F2 F3 result的对应关系没错。。不要在意细节~
1 然后请分类（c,T,0.8）要求连续变量离散化？是这个？反正我不清楚，然后乱写了一通。。。  ？ 连续变量离散化？
2 问你用2-NN算法分类(c,T,0.8)这个很简单啦，肯定算出来是离5和6最近。。

六 20分
设计一个机器学习算法，类似往年题，会给出你要设计什么，然后结合你对课程的理解YY就行了。
